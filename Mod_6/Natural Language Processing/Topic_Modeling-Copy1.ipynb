{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is Topic Modeling and NLP useful?\n",
    "\n",
    "- Imagine you have take all the songs that made the Top 100 billboard Charts since 1970. How could you then use topic modeling and NLP to create some  useful analysis?\n",
    "\n",
    "\n",
    "- You work for a online news aggregator that takes news articles from all over the internet and sends out important articles to the users. The problem is each website has a different way of classifying their news stories. How could topic modeling and NLP to help you?\n",
    "\n",
    "\n",
    "- You work for Ted Talks and want to predict the number of views each video will get.  Right now you only have the text of the speeches, their length, and the number of views. How could you use NLP and topic modelling to help with your predictive model?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/54/1d7294672110d5c0565cabc4b99ed952ced9a2dc2ca1d59ad1b34303a6de/gensim-3.8.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (24.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.7MB 1.1MB/s eta 0:00:01   17% |█████▋                          | 4.3MB 19.9MB/s eta 0:00:02    38% |████████████▎                   | 9.5MB 25.2MB/s eta 0:00:01    58% |██████████████████▊             | 14.4MB 26.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /anaconda3/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/09/735f2786dfac9bbf39d244ce75c0313d27d4962e71e0774750dc809f2395/smart_open-1.9.0.tar.gz (70kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 10.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /anaconda3/lib/python3.7/site-packages (from gensim) (1.16.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /anaconda3/lib/python3.7/site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied: boto>=2.32 in /anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.21.0)\n",
      "Collecting boto3 (from smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/c4/d777bd125e6ef304ef52aa7c442a39b33b491922c6495ebd6663322508e4/boto3-1.10.10-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 25.3MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.3.9)\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from boto3->smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 2.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.14.0,>=1.13.10 (from boto3->smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/69/78a1ee8d8c302c4cee6b088f2d166686b78135a86a65c5d2207b2964e438/botocore-1.13.10-py2.py3-none-any.whl (5.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.4MB 2.9MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /anaconda3/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.10->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /anaconda3/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.10->boto3->smart-open>=1.8.1->gensim) (0.14)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/justintennenbaum/Library/Caches/pip/wheels/ab/10/93/5cff86f5b721d77edaecc29959b1c60d894be1f66d91407d28\n",
      "Successfully built smart-open\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.10.10 botocore-1.13.10 gensim-3.8.1 jmespath-0.9.4 s3transfer-0.2.1 smart-open-1.9.0\n",
      "\u001b[33mYou are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/justintennenbaum/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.autos' 'comp.sys.mac.hardware' 'rec.motorcycles' 'misc.forsale'\n",
      " 'comp.os.ms-windows.misc' 'alt.atheism' 'comp.graphics'\n",
      " 'rec.sport.baseball' 'rec.sport.hockey' 'sci.electronics' 'sci.space'\n",
      " 'talk.politics.misc' 'sci.med' 'talk.politics.mideast'\n",
      " 'soc.religion.christian' 'comp.windows.x' 'comp.sys.ibm.pc.hardware'\n",
      " 'talk.politics.guns' 'talk.religion.misc' 'sci.crypt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>From: irwin@cmptrc.lonestar.org (Irwin Arnstei...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>From: tchen@magnus.acs.ohio-state.edu (Tsung-K...</td>\n",
       "      <td>6</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>From: a207706@moe.dseg.ti.com (Robert Loper)\\n...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>From: kimman@magnus.acs.ohio-state.edu (Kim Ri...</td>\n",
       "      <td>6</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>From: kwilson@casbah.acns.nwu.edu (Kirtley Wil...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>Subject: Re: Don't more innocents die without ...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>From: livesey@solntze.wpd.sgi.com (Jon Livesey...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>From: dls@aeg.dsto.gov.au (David Silver)\\nSubj...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10006</th>\n",
       "      <td>Subject: Re: Mike Francesa's 1993 Predictions\\...</td>\n",
       "      <td>9</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10007</th>\n",
       "      <td>From: jet@netcom.Netcom.COM (J. Eric Townsend)...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10008</th>\n",
       "      <td>From: gld@cunixb.cc.columbia.edu (Gary L Dare)...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>From: sehari@iastate.edu (Babak Sehari)\\nSubje...</td>\n",
       "      <td>12</td>\n",
       "      <td>sci.electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>From: danmg@grok85.ColumbiaSC.NCR.COM (Daniel ...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>From: henry@zoo.toronto.edu (Henry Spencer)\\nS...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10011</th>\n",
       "      <td>From: tzs@stein2.u.washington.edu (Tim Smith)\\...</td>\n",
       "      <td>18</td>\n",
       "      <td>talk.politics.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10012</th>\n",
       "      <td>From: U56149@uicvm.uic.edu\\nSubject: LCIII &amp; M...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10013</th>\n",
       "      <td>From: nsmca@aurora.alaska.edu\\nSubject: Lunar ...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10014</th>\n",
       "      <td>From: acooper@mac.cc.macalstr.edu\\nSubject: Re...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10015</th>\n",
       "      <td>From: billc@col.hp.com (Bill Claussen)\\nSubjec...</td>\n",
       "      <td>13</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10016</th>\n",
       "      <td>From: vestman@cs.umu.se (Peter Vestman)\\nSubje...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10017</th>\n",
       "      <td>From: nstramer@supergas.dazixco.ingr.com (Naft...</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10018</th>\n",
       "      <td>From: kohli@ecs.umass.edu\\nSubject: Mazda GLC ...</td>\n",
       "      <td>6</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10019</th>\n",
       "      <td>From: mussack@austin.ibm.com (Christopher Muss...</td>\n",
       "      <td>15</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>From: PA146008@utkvm1.utk.edu (David Veal)\\nSu...</td>\n",
       "      <td>18</td>\n",
       "      <td>talk.politics.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10020</th>\n",
       "      <td>From: noye@midway.uchicago.edu (vera shanti no...</td>\n",
       "      <td>15</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10021</th>\n",
       "      <td>From:  (Sean Garrison)\\nSubject: Re: WFAN\\nNnt...</td>\n",
       "      <td>9</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10022</th>\n",
       "      <td>From: christy@cs.concordia.ca (Christy)\\nSubje...</td>\n",
       "      <td>5</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  target  \\\n",
       "0      From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1      From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "10     From: irwin@cmptrc.lonestar.org (Irwin Arnstei...       8   \n",
       "100    From: tchen@magnus.acs.ohio-state.edu (Tsung-K...       6   \n",
       "1000   From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...       2   \n",
       "10000  From: a207706@moe.dseg.ti.com (Robert Loper)\\n...       7   \n",
       "10001  From: kimman@magnus.acs.ohio-state.edu (Kim Ri...       6   \n",
       "10002  From: kwilson@casbah.acns.nwu.edu (Kirtley Wil...       2   \n",
       "10003  Subject: Re: Don't more innocents die without ...       0   \n",
       "10004  From: livesey@solntze.wpd.sgi.com (Jon Livesey...       0   \n",
       "10005  From: dls@aeg.dsto.gov.au (David Silver)\\nSubj...       1   \n",
       "10006  Subject: Re: Mike Francesa's 1993 Predictions\\...       9   \n",
       "10007  From: jet@netcom.Netcom.COM (J. Eric Townsend)...       8   \n",
       "10008  From: gld@cunixb.cc.columbia.edu (Gary L Dare)...      10   \n",
       "10009  From: sehari@iastate.edu (Babak Sehari)\\nSubje...      12   \n",
       "1001   From: danmg@grok85.ColumbiaSC.NCR.COM (Daniel ...       7   \n",
       "10010  From: henry@zoo.toronto.edu (Henry Spencer)\\nS...      14   \n",
       "10011  From: tzs@stein2.u.washington.edu (Tim Smith)\\...      18   \n",
       "10012  From: U56149@uicvm.uic.edu\\nSubject: LCIII & M...       4   \n",
       "10013  From: nsmca@aurora.alaska.edu\\nSubject: Lunar ...      14   \n",
       "10014  From: acooper@mac.cc.macalstr.edu\\nSubject: Re...       0   \n",
       "10015  From: billc@col.hp.com (Bill Claussen)\\nSubjec...      13   \n",
       "10016  From: vestman@cs.umu.se (Peter Vestman)\\nSubje...       2   \n",
       "10017  From: nstramer@supergas.dazixco.ingr.com (Naft...      17   \n",
       "10018  From: kohli@ecs.umass.edu\\nSubject: Mazda GLC ...       6   \n",
       "10019  From: mussack@austin.ibm.com (Christopher Muss...      15   \n",
       "1002   From: PA146008@utkvm1.utk.edu (David Veal)\\nSu...      18   \n",
       "10020  From: noye@midway.uchicago.edu (vera shanti no...      15   \n",
       "10021  From:  (Sean Garrison)\\nSubject: Re: WFAN\\nNnt...       9   \n",
       "10022  From: christy@cs.concordia.ca (Christy)\\nSubje...       5   \n",
       "\n",
       "                  target_names  \n",
       "0                    rec.autos  \n",
       "1        comp.sys.mac.hardware  \n",
       "10             rec.motorcycles  \n",
       "100               misc.forsale  \n",
       "1000   comp.os.ms-windows.misc  \n",
       "10000                rec.autos  \n",
       "10001             misc.forsale  \n",
       "10002  comp.os.ms-windows.misc  \n",
       "10003              alt.atheism  \n",
       "10004              alt.atheism  \n",
       "10005            comp.graphics  \n",
       "10006       rec.sport.baseball  \n",
       "10007          rec.motorcycles  \n",
       "10008         rec.sport.hockey  \n",
       "10009          sci.electronics  \n",
       "1001                 rec.autos  \n",
       "10010                sci.space  \n",
       "10011       talk.politics.misc  \n",
       "10012    comp.sys.mac.hardware  \n",
       "10013                sci.space  \n",
       "10014              alt.atheism  \n",
       "10015                  sci.med  \n",
       "10016  comp.os.ms-windows.misc  \n",
       "10017    talk.politics.mideast  \n",
       "10018             misc.forsale  \n",
       "10019   soc.religion.christian  \n",
       "1002        talk.politics.misc  \n",
       "10020   soc.religion.christian  \n",
       "10021       rec.sport.baseball  \n",
       "10022           comp.windows.x  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Import Dataset\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "print(df.target_names.unique())\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Explanation Resources \n",
    "\n",
    "https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158\n",
    "\n",
    "https://www.youtube.com/watch?v=DWJYZq_fQ2A\n",
    "\n",
    "https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.machinelearningplus.com/wp-content/uploads/2018/03/Inferring-Topic-from-Keywords-1024x666.png' img/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d8b43408e7b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize(text, pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    word = WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "#     print('token',word)\n",
    "    return stemmer.stem(word)\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a document to preview after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['From:', 'lerxst@wam.umd.edu', \"(where's\", 'my', 'thing)\\nSubject:', 'WHAT', 'car', 'is', 'this!?\\nNntp-Posting-Host:', 'rac3.wam.umd.edu\\nOrganization:', 'University', 'of', 'Maryland,', 'College', 'Park\\nLines:', '15\\n\\n', 'I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw\\nthe', 'other', 'day.', 'It', 'was', 'a', '2-door', 'sports', 'car,', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/\\nearly', '70s.', 'It', 'was', 'called', 'a', 'Bricklin.', 'The', 'doors', 'were', 'really', 'small.', 'In', 'addition,\\nthe', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body.', 'This', 'is', '\\nall', 'I', 'know.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name,', 'engine', 'specs,', 'years\\nof', 'production,', 'where', 'this', 'car', 'is', 'made,', 'history,', 'or', 'whatever', 'info', 'you\\nhave', 'on', 'this', 'funky', 'looking', 'car,', 'please', 'e-mail.\\n\\nThanks,\\n-', 'IL\\n', '', '', '----', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst', '----\\n\\n\\n\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['lerxst', 'thing', 'subject', 'nntp', 'post', 'host', 'organ', 'univers', 'maryland', 'colleg', 'park', 'line', 'wonder', 'enlighten', 'door', 'sport', 'look', 'late', 'earli', 'call', 'bricklin', 'door', 'small', 'addit', 'bumper', 'separ', 'rest', 'bodi', 'know', 'tellm', 'model', 'engin', 'spec', 'year', 'product', 'histori', 'info', 'funki', 'look', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = df.iloc[0].values[0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = df['content'].map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [lerxst, thing, subject, nntp, post, host, org...\n",
       "1        [guykuo, carson, washington, subject, clock, p...\n",
       "10       [irwin, cmptrc, lonestar, irwin, arnstein, sub...\n",
       "100      [tchen, magnus, ohio, state, tsung, chen, subj...\n",
       "1000     [dabl, lindbergh, subject, diamond, mous, curs...\n",
       "10000    [dseg, robert, loper, subject, nntp, post, hos...\n",
       "10001    [kimman, magnus, ohio, state, richard, subject...\n",
       "10002    [kwilson, casbah, acn, kirtley, wilson, subjec...\n",
       "10003    [subject, innoc, death, penalti, bobb, vice, r...\n",
       "10004    [livesey, solntz, livesey, subject, genocid, c...\n",
       "10005    [dsto, david, silver, subject, fractal, genera...\n",
       "10006    [subject, mike, francesa, predict, gajarski, p...\n",
       "10007    [netcom, netcom, eric, townsend, subject, insu...\n",
       "10008    [cunixb, columbia, gari, dare, subject, covera...\n",
       "10009    [sehari, iastat, babak, sehari, subject, disk,...\n",
       "1001     [danmg, grok, columbiasc, daniel, adam, subjec...\n",
       "10010    [henri, toronto, henri, spencer, subject, luna...\n",
       "10011    [stein, washington, smith, subject, tie, abort...\n",
       "10012    [uicvm, subject, lciii, midi, articl, uicvm, o...\n",
       "10013    [nsmca, aurora, alaska, subject, lunar, coloni...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words on the Data set\n",
    "Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 addit\n",
      "1 bodi\n",
      "2 bricklin\n",
      "3 bring\n",
      "4 bumper\n",
      "5 call\n",
      "6 colleg\n",
      "7 door\n",
      "8 earli\n",
      "9 engin\n",
      "10 enlighten\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out tokens that appear in:\n",
    "\n",
    "- less than 15 documents (absolute number) \n",
    "- more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
    "- after the above two steps, keep only the first 100000 most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=20, no_above=0.66, keep_n=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5433"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim doc2bow\n",
    "\n",
    "For each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, 1),\n",
       " (20, 1),\n",
       " (22, 1),\n",
       " (27, 1),\n",
       " (31, 1),\n",
       " (138, 1),\n",
       " (141, 3),\n",
       " (237, 1),\n",
       " (242, 1),\n",
       " (268, 5),\n",
       " (395, 2),\n",
       " (580, 1),\n",
       " (585, 1),\n",
       " (614, 1),\n",
       " (641, 1),\n",
       " (685, 1),\n",
       " (793, 1),\n",
       " (814, 1),\n",
       " (855, 1),\n",
       " (897, 1),\n",
       " (960, 1),\n",
       " (1020, 1),\n",
       " (1341, 1),\n",
       " (1350, 1),\n",
       " (1510, 1),\n",
       " (1556, 1),\n",
       " (1619, 1),\n",
       " (1767, 2),\n",
       " (1823, 1),\n",
       " (1924, 1),\n",
       " (2383, 1),\n",
       " (2526, 1),\n",
       " (2568, 1),\n",
       " (2690, 1),\n",
       " (2788, 1),\n",
       " (4150, 1),\n",
       " (4326, 1),\n",
       " (4530, 2),\n",
       " (5013, 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview Bag Of Words for our sample preprocessed document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 11 (\"host\") appears 1 time.\n",
      "Word 20 (\"nntp\") appears 1 time.\n",
      "Word 22 (\"post\") appears 1 time.\n",
      "Word 27 (\"spec\") appears 1 time.\n",
      "Word 31 (\"univers\") appears 1 time.\n",
      "Word 138 (\"state\") appears 1 time.\n",
      "Word 141 (\"window\") appears 3 time.\n",
      "Word 237 (\"write\") appears 1 time.\n",
      "Word 242 (\"richard\") appears 1 time.\n",
      "Word 268 (\"program\") appears 5 time.\n",
      "Word 395 (\"silver\") appears 2 time.\n",
      "Word 580 (\"secur\") appears 1 time.\n",
      "Word 585 (\"true\") appears 1 time.\n",
      "Word 614 (\"gate\") appears 1 time.\n",
      "Word 641 (\"econom\") appears 1 time.\n",
      "Word 685 (\"high\") appears 1 time.\n",
      "Word 793 (\"support\") appears 1 time.\n",
      "Word 814 (\"meet\") appears 1 time.\n",
      "Word 855 (\"correct\") appears 1 time.\n",
      "Word 897 (\"task\") appears 1 time.\n",
      "Word 960 (\"current\") appears 1 time.\n",
      "Word 1020 (\"major\") appears 1 time.\n",
      "Word 1341 (\"oper\") appears 1 time.\n",
      "Word 1350 (\"promis\") appears 1 time.\n",
      "Word 1510 (\"dept\") appears 1 time.\n",
      "Word 1556 (\"server\") appears 1 time.\n",
      "Word 1619 (\"user\") appears 1 time.\n",
      "Word 1767 (\"multi\") appears 2 time.\n",
      "Word 1823 (\"expect\") appears 1 time.\n",
      "Word 1924 (\"assur\") appears 1 time.\n",
      "Word 2383 (\"jose\") appears 1 time.\n",
      "Word 2526 (\"processor\") appears 1 time.\n",
      "Word 2568 (\"math\") appears 1 time.\n",
      "Word 2690 (\"rick\") appears 1 time.\n",
      "Word 2788 (\"overhead\") appears 1 time.\n",
      "Word 4150 (\"giant\") appears 1 time.\n",
      "Word 4326 (\"primarili\") appears 1 time.\n",
      "Word 4530 (\"billi\") appears 2 time.\n",
      "Word 5013 (\"app\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.16523076661355718),\n",
      " (1, 0.1677664905617056),\n",
      " (2, 0.15012097927517018),\n",
      " (3, 0.2856620819895799),\n",
      " (4, 0.11433072900048068),\n",
      " (5, 0.15159954267776288),\n",
      " (6, 0.38912221066291464),\n",
      " (7, 0.16881835532183245),\n",
      " (8, 0.12272982017825479),\n",
      " (9, 0.2633178454540476),\n",
      " (10, 0.16437312316877342),\n",
      " (11, 0.041885373029623627),\n",
      " (12, 0.13936358719351144),\n",
      " (13, 0.05320037340398117),\n",
      " (14, 0.17831230414471078),\n",
      " (15, 0.1613726005913341),\n",
      " (16, 0.1017696733142144),\n",
      " (17, 0.23327140545553357),\n",
      " (18, 0.16211755838097908),\n",
      " (19, 0.2803124591711666),\n",
      " (20, 0.042624920597482026),\n",
      " (21, 0.1848445683855343),\n",
      " (22, 0.03254024351960623),\n",
      " (23, 0.14859699959830053),\n",
      " (24, 0.15962827481075148),\n",
      " (25, 0.1814706209577019),\n",
      " (26, 0.14469438092060538),\n",
      " (27, 0.2096099743383186),\n",
      " (28, 0.1928032035466336),\n",
      " (29, 0.08427773412644116),\n",
      " (30, 0.08372804371108514),\n",
      " (31, 0.04439481147976632),\n",
      " (32, 0.13714379483744416),\n",
      " (33, 0.0841337463198108)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using Bag of Words\n",
    "\n",
    "Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=4, workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each topic, we will explore the words occuring in that topic and its relative weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.026*\"file\" + 0.016*\"entri\" + 0.010*\"list\" + 0.010*\"send\" + 0.009*\"program\" + 0.009*\"output\" + 0.008*\"section\" + 0.007*\"request\" + 0.007*\"period\" + 0.007*\"firearm\"\n",
      "Topic: 1 \n",
      "Words: 0.018*\"encrypt\" + 0.015*\"chip\" + 0.012*\"clipper\" + 0.011*\"secur\" + 0.009*\"govern\" + 0.007*\"key\" + 0.007*\"escrow\" + 0.006*\"inform\" + 0.006*\"phone\" + 0.006*\"write\"\n",
      "Topic: 2 \n",
      "Words: 0.010*\"write\" + 0.009*\"post\" + 0.009*\"articl\" + 0.008*\"know\" + 0.007*\"good\" + 0.007*\"drug\" + 0.006*\"help\" + 0.006*\"univers\" + 0.006*\"like\" + 0.006*\"time\"\n",
      "Topic: 3 \n",
      "Words: 0.016*\"write\" + 0.015*\"articl\" + 0.011*\"univers\" + 0.010*\"post\" + 0.008*\"uiuc\" + 0.008*\"year\" + 0.008*\"host\" + 0.007*\"nntp\" + 0.007*\"bike\" + 0.006*\"know\"\n",
      "Topic: 4 \n",
      "Words: 0.021*\"space\" + 0.016*\"nasa\" + 0.008*\"orbit\" + 0.006*\"launch\" + 0.005*\"year\" + 0.005*\"center\" + 0.005*\"research\" + 0.005*\"data\" + 0.005*\"imag\" + 0.005*\"satellit\"\n",
      "Topic: 5 \n",
      "Words: 0.011*\"think\" + 0.010*\"write\" + 0.010*\"exist\" + 0.009*\"moral\" + 0.009*\"peopl\" + 0.008*\"christian\" + 0.008*\"know\" + 0.007*\"atheist\" + 0.007*\"believ\" + 0.007*\"thing\"\n",
      "Topic: 6 \n",
      "Words: 0.027*\"jesus\" + 0.011*\"write\" + 0.010*\"christian\" + 0.010*\"christ\" + 0.008*\"bibl\" + 0.008*\"say\" + 0.007*\"faith\" + 0.007*\"come\" + 0.007*\"believ\" + 0.007*\"john\"\n",
      "Topic: 7 \n",
      "Words: 0.011*\"state\" + 0.011*\"peopl\" + 0.009*\"israel\" + 0.009*\"write\" + 0.008*\"presid\" + 0.008*\"articl\" + 0.008*\"isra\" + 0.007*\"know\" + 0.007*\"think\" + 0.006*\"jew\"\n",
      "Topic: 8 \n",
      "Words: 0.036*\"window\" + 0.013*\"widget\" + 0.010*\"manag\" + 0.010*\"applic\" + 0.007*\"like\" + 0.007*\"post\" + 0.007*\"user\" + 0.007*\"event\" + 0.006*\"program\" + 0.006*\"code\"\n",
      "Topic: 9 \n",
      "Words: 0.013*\"govern\" + 0.012*\"post\" + 0.012*\"write\" + 0.009*\"articl\" + 0.006*\"right\" + 0.006*\"nntp\" + 0.006*\"libertarian\" + 0.006*\"host\" + 0.006*\"clinton\" + 0.006*\"like\"\n",
      "Topic: 10 \n",
      "Words: 0.010*\"write\" + 0.009*\"like\" + 0.008*\"time\" + 0.007*\"good\" + 0.007*\"think\" + 0.007*\"articl\" + 0.005*\"engin\" + 0.005*\"go\" + 0.004*\"peopl\" + 0.004*\"post\"\n",
      "Topic: 11 \n",
      "Words: 0.020*\"game\" + 0.017*\"team\" + 0.013*\"play\" + 0.011*\"player\" + 0.011*\"year\" + 0.008*\"hockey\" + 0.008*\"season\" + 0.007*\"think\" + 0.007*\"like\" + 0.007*\"write\"\n",
      "Topic: 12 \n",
      "Words: 0.007*\"program\" + 0.007*\"softwar\" + 0.007*\"graphic\" + 0.007*\"like\" + 0.007*\"version\" + 0.006*\"write\" + 0.006*\"avail\" + 0.006*\"includ\" + 0.006*\"card\" + 0.005*\"need\"\n",
      "Topic: 13 \n",
      "Words: 0.014*\"wire\" + 0.014*\"scsi\" + 0.012*\"write\" + 0.010*\"articl\" + 0.009*\"drive\" + 0.007*\"power\" + 0.007*\"know\" + 0.006*\"grind\" + 0.006*\"like\" + 0.006*\"work\"\n",
      "Topic: 14 \n",
      "Words: 0.033*\"post\" + 0.026*\"host\" + 0.026*\"nntp\" + 0.016*\"univers\" + 0.012*\"articl\" + 0.011*\"write\" + 0.010*\"repli\" + 0.010*\"thank\" + 0.010*\"distribut\" + 0.007*\"andrew\"\n",
      "Topic: 15 \n",
      "Words: 0.013*\"peopl\" + 0.010*\"armenian\" + 0.010*\"say\" + 0.008*\"know\" + 0.007*\"think\" + 0.006*\"come\" + 0.006*\"right\" + 0.006*\"go\" + 0.005*\"time\" + 0.005*\"turkish\"\n",
      "Topic: 16 \n",
      "Words: 0.017*\"problem\" + 0.017*\"window\" + 0.015*\"drive\" + 0.012*\"file\" + 0.010*\"work\" + 0.010*\"card\" + 0.010*\"disk\" + 0.009*\"driver\" + 0.008*\"write\" + 0.008*\"univers\"\n",
      "Topic: 17 \n",
      "Words: 0.016*\"write\" + 0.013*\"articl\" + 0.009*\"think\" + 0.008*\"appl\" + 0.008*\"like\" + 0.007*\"truth\" + 0.007*\"univers\" + 0.007*\"church\" + 0.007*\"peopl\" + 0.006*\"post\"\n",
      "Topic: 18 \n",
      "Words: 0.017*\"write\" + 0.017*\"articl\" + 0.017*\"pitt\" + 0.014*\"columbia\" + 0.012*\"bank\" + 0.011*\"gordon\" + 0.009*\"univers\" + 0.007*\"post\" + 0.007*\"like\" + 0.006*\"repli\"\n",
      "Topic: 19 \n",
      "Words: 0.011*\"write\" + 0.010*\"articl\" + 0.008*\"peopl\" + 0.006*\"think\" + 0.006*\"kill\" + 0.006*\"like\" + 0.006*\"right\" + 0.005*\"post\" + 0.005*\"weapon\" + 0.005*\"israel\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=20, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model\n",
    "We will check where our test document would be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model on unseen document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_document = ''\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\n Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Model Perplexity and Coherence Score\n",
    "Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is.  Topic coherence score, in particular, has been more helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.769944469800711\n",
      "\n",
      "Coherence Score:  0.43388944506097127\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(bow_corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "    https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing this with sklearn\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/\n",
    "\n",
    "https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
